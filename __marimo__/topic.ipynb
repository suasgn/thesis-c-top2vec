{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.15 (main, Sep  9 2024, 22:43:48) [Clang 18.1.8 ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import marimo as mo\n",
    "import re\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# modified top2vec library\n",
    "from _top2vec import Top2Vec\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if cuda or mps available, if available, use one of them, otherwise use cpu\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"using cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     # os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = (\n",
    "#     #     \"1\"  # This is tracked as pytorch issue #98222\n",
    "#     # )\n",
    "#     print(\"using mps\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _detokenize_sentence(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return \" \".join([t for t in tokens if isinstance(t, str)]).strip()\n",
    "    if isinstance(tokens, str):\n",
    "        return tokens.strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_full_paragraph_and_summary(data):\n",
    "    paragraph_sentences = []\n",
    "    summary_sentences = []\n",
    "\n",
    "    for each_paragraph in data[\"paragraphs\"]:\n",
    "        for each_sentence in each_paragraph:\n",
    "            paragraph_sentences.append(_detokenize_sentence(each_sentence))\n",
    "\n",
    "    for each_summary in data[\"summary\"]:\n",
    "        summary_sentences.append(_detokenize_sentence(each_summary))\n",
    "\n",
    "    paragraph = \" \".join([s for s in paragraph_sentences if s]).strip()\n",
    "    summary = \" \".join([s for s in summary_sentences if s]).strip()\n",
    "    return {\"document\": paragraph, \"summary\": summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class='text-xs'>DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: [&#x27;summary&#x27;, &#x27;document&#x27;],\n",
       "        num_rows: 14262\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: [&#x27;summary&#x27;, &#x27;document&#x27;],\n",
       "        num_rows: 750\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: [&#x27;summary&#x27;, &#x27;document&#x27;],\n",
       "        num_rows: 3762\n",
       "    })\n",
       "})</pre>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"joshuasiagian/indosum\")\n",
    "\n",
    "ds = ds.map(\n",
    "    get_full_paragraph_and_summary, remove_columns=ds[\"train\"].column_names\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"summary\": [\n",
      "        \"Dokter Lula Kamal yang merupakan selebriti sekaligus rekan kerja Ryan Thamrin menyebut kawannya itu sudah sakit sejak setahun yang lalu . Lula menuturkan , sakit itu membuat Ryan mesti vakum dari semua kegiatannya , termasuk menjadi pembawa acara Dokter Oz Indonesia . Kondisi itu membuat Ryan harus kembali ke kampung halamannya di Pekanbaru , Riau untuk menjalani istirahat .\"\n",
      "    ],\n",
      "    \"document\": [\n",
      "        \"Jakarta , CNN Indonesia - - Dokter Ryan Thamrin , yang terkenal lewat acara Dokter Oz Indonesia , meninggal dunia pada Jumat ( 4 / 8 ) dini hari . Dokter Lula Kamal yang merupakan selebriti sekaligus rekan kerja Ryan menyebut kawannya itu sudah sakit sejak setahun yang lalu . Lula menuturkan , sakit itu membuat Ryan mesti vakum dari semua kegiatannya , termasuk menjadi pembawa acara Dokter Oz Indonesia . Kondisi itu membuat Ryan harus kembali ke kampung halamannya di Pekanbaru , Riau untuk menjalani istirahat . \\\" Setahu saya dia orangnya sehat , tapi tahun lalu saya dengar dia sakit . ( Karena ) sakitnya , ia langsung pulang ke Pekanbaru , jadi kami yang mau jenguk juga susah . Barangkali mau istirahat , ya betul juga , kalau di Jakarta susah isirahatnya , \\\" kata Lula kepada CNNIndonesia.com , Jumat ( 4 / 8 ) . Lula yang mengenal Ryan sejak sebelum aktif berkarier di televisi mengaku belum sempat membesuk Ryan lantaran lokasi yang jauh . Dia juga tak tahu penyakit apa yang diderita Ryan . \\\" Itu saya enggak tahu , belum sempat jenguk dan enggak selamanya bisa dijenguk juga . Enggak tahu berat sekali apa bagaimana , \\\" tutur Ryan . Walau sudah setahun menderita sakit , Lula tak mengetahui apa penyebab pasti kematian Dr Oz Indonesia itu . Meski demikian , ia mendengar beberapa kabar yang menyebut bahwa penyebab Ryan meninggal adalah karena jatuh di kamar mandi . \\u201c Saya tidak tahu , barangkali penyakit yang dulu sama yang sekarang berbeda , atau penyebab kematiannya beda dari penyakit sebelumnya . Kita kan enggak bisa mengambil kesimpulan , \\\" kata Lula . Ryan Thamrin terkenal sebagai dokter yang rutin membagikan tips dan informasi kesehatan lewat tayangan Dokter Oz Indonesia . Ryan menempuh Pendidikan Dokter pada tahun 2002 di Fakultas Kedokteran Universitas Gadjah Mada . Dia kemudian melanjutkan pendidikan Klinis Kesehatan Reproduksi dan Penyakit Menular Seksual di Mahachulalongkornrajavidyalaya University , Bangkok , Thailand pada 2004 .\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# explore the first 5 data in the dataset\n",
    "print(json.dumps(ds[\"train\"][:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 12:50:14,317 - top2vec - INFO - Pre-processing documents for training\n",
      "/Users/siagian/thesis/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2026-02-05 12:50:23,244 - top2vec - INFO - Creating vocabulary embedding\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "sentence-transformers/paraphrase-multilingual-MiniLM-L6-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 403, in hf_raise_for_status",
      "    response.raise_for_status()",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/requests/models.py\", line 1026, in raise_for_status",
      "    raise HTTPError(http_error_msg, response=self)",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L6-v2/resolve/main/tokenizer_config.json",
      "",
      "The above exception was the direct cause of the following exception:",
      "",
      "Traceback (most recent call last):",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/transformers/utils/hub.py\", line 479, in cached_files",
      "    hf_hub_download(",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn",
      "    return fn(*args, **kwargs)",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1007, in hf_hub_download",
      "    return _hf_hub_download_to_cache_dir(",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1114, in _hf_hub_download_to_cache_dir",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1655, in _raise_on_head_call_error",
      "    raise head_call_error",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1543, in _get_metadata_or_catch_error",
      "    metadata = get_hf_file_metadata(",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn",
      "    return fn(*args, **kwargs)",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1460, in get_hf_file_metadata",
      "    r = _request_wrapper(",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 283, in _request_wrapper",
      "    response = _request_wrapper(",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 307, in _request_wrapper",
      "    hf_raise_for_status(response)",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 453, in hf_raise_for_status",
      "    raise _format(RepositoryNotFoundError, message, response) from e",
      "huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-69842fa0-7c218531255839f23367aecc;1c6c6b1b-0812-4abd-818c-07b401884274)",
      "",
      "Repository Not Found for url: https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L6-v2/resolve/main/tokenizer_config.json.",
      "Please make sure you specified the correct `repo_id` and `repo_type`.",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
      "",
      "The above exception was the direct cause of the following exception:",
      "",
      "Traceback (most recent call last):",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/marimo/_runtime/executor.py\", line 138, in execute_cell",
      "    exec(cell.body, glbls)",
      "  File \"\", line 10, in <module>",
      "    top2vec_model = Top2Vec(",
      "  File \"/Users/siagian/thesis/_top2vec/top2vec.py\", line 740, in __init__",
      "    self.word_vectors = average_embeddings(self.vocab,",
      "  File \"/Users/siagian/thesis/_top2vec/embedding.py\", line 13, in average_embeddings",
      "    tokenizer = AutoTokenizer.from_pretrained(embedding_model)",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 1089, in from_pretrained",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 921, in get_tokenizer_config",
      "    resolved_config_file = cached_file(",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/transformers/utils/hub.py\", line 322, in cached_file",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)",
      "  File \"/Users/siagian/thesis/.venv/lib/python3.10/site-packages/transformers/utils/hub.py\", line 511, in cached_files",
      "    raise OSError(",
      "OSError: sentence-transformers/paraphrase-multilingual-MiniLM-L6-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
      ""
     ]
    }
   ],
   "source": [
    "documents = ds[\"train\"][\"document\"]\n",
    "\n",
    "# Basic cleaning: strip whitespace and remove empty entries\n",
    "documents = [\n",
    "    d.strip() for d in documents if isinstance(d, str) and len(d.strip()) > 0\n",
    "]\n",
    "\n",
    "len(documents)\n",
    "\n",
    "top2vec_model = Top2Vec(\n",
    "    documents=documents,\n",
    "    ngram_vocab=True,\n",
    "    contextual_top2vec=True,\n",
    "    # embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\",  # modified top2vec. The original top2vec only supports \"all-MiniLM-L6-v2\" and \"all-mpnet-base-v2\"\n",
    "    embedding_model=\"paraphrase-multilingual-MiniLM-L6-v2\",\n",
    ")\n",
    "\n",
    "top2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [
    {
     "ename": "Ancestor raised",
     "evalue": "An ancestor raised an exception (OSError)",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "def _():\n",
    "    # Inspect topics learned by the model\n",
    "    num_topics = top2vec_model.get_num_topics()\n",
    "    topic_sizes, topic_nums = top2vec_model.get_topic_sizes()\n",
    "    top_terms_per_topic = []\n",
    "    for topic_num in topic_nums:\n",
    "        words, word_scores, _ = top2vec_model.get_topics(topic_num)\n",
    "        top_terms_per_topic.append(\n",
    "            {\n",
    "                \"topic\": int(topic_num),\n",
    "                \"size\": int(topic_sizes[topic_nums.tolist().index(topic_num)]),\n",
    "                \"top_terms\": words[:10],\n",
    "                \"term_scores\": word_scores[:10].tolist(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Display a compact summary\n",
    "    summary = {\n",
    "        \"num_topics\": int(num_topics),\n",
    "        \"largest_topics\": [\n",
    "            {\n",
    "                \"topic\": int(topic_nums[i]),\n",
    "                \"size\": int(topic_sizes[i]),\n",
    "                \"top_terms\": top_terms_per_topic[i][\"top_terms\"],\n",
    "            }\n",
    "            for i in range(min(10, len(topic_nums)))\n",
    "        ],\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [
    {
     "ename": "Ancestor raised",
     "evalue": "An ancestor raised an exception (OSError)",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# Save the trained model for later reuse\n",
    "model_path = \"models/top2vec_indosum_multilingual_l6\"\n",
    "top2vec_model.save(model_path)\n",
    "{\"saved_path\": model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [
    {
     "ename": "Ancestor raised",
     "evalue": "An ancestor raised an exception (OSError)",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# Count documents with top topic score higher/lower than 0.7\n",
    "threshold = 0.5\n",
    "high_score_count = 0\n",
    "low_score_count = 0\n",
    "\n",
    "docs = None\n",
    "if hasattr(top2vec_model, \"documents\") and top2vec_model.documents is not None:\n",
    "    docs = top2vec_model.documents\n",
    "else:\n",
    "    try:\n",
    "        docs, _, _ = top2vec_model.get_documents(\n",
    "            list(range(top2vec_model.get_num_documents()))\n",
    "        )\n",
    "    except Exception:\n",
    "        docs = None\n",
    "\n",
    "if docs is None:\n",
    "    raise ValueError(\n",
    "        \"Cannot access documents from model. Ensure keep_documents=True when training.\"\n",
    "    )\n",
    "\n",
    "topic_dist = top2vec_model.get_document_topic_distribution()\n",
    "# topic_sizes, topic_nums = top2vec_model.get_topic_sizes()\n",
    "\n",
    "for _doc_id in range(len(docs)):\n",
    "    _dist = topic_dist[_doc_id]\n",
    "    top_score = _dist.max()\n",
    "    if top_score > threshold:\n",
    "        high_score_count += 1\n",
    "    else:\n",
    "        low_score_count += 1\n",
    "\n",
    "print(f\"Documents with top topic score > {threshold}: {high_score_count}\")\n",
    "print(f\"Documents with top topic score <= {threshold}: {low_score_count}\")\n",
    "print(f\"Total documents: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {},
   "source": [
    "## Embedding models\n",
    "\n",
    "### paraphrase-multilingual-MiniLM-L12-v2\n",
    "\n",
    "Documents with top topic score > 0.5: 10569\n",
    "Documents with top topic score <= 0.5: 3693\n",
    "Total documents: 14262\n",
    "\n",
    "Documents with top topic score > 0.7: 7432\n",
    "Documents with top topic score <= 0.7: 6830\n",
    "Total documents: 14262\n",
    "\n",
    "### all-MiniLM-L6-v2\n",
    "\n",
    "Documents with top topic score > 0.5: 4248\n",
    "Documents with top topic score <= 0.5: 10014\n",
    "Total documents: 14262\n",
    "\n",
    "\n",
    "Documents with top topic score > 0.7: 1160\n",
    "Documents with top topic score <= 0.7: 13102\n",
    "Total documents: 14262"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "marimo": {
   "app_config": {
    "auto_download": [
     "html",
     "ipynb"
    ],
    "width": "full"
   },
   "marimo_version": "0.19.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
